{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "419fb707",
   "metadata": {},
   "source": [
    "# Data Tokenization\n",
    "\n",
    "Tokenization is a technique where sensitive data is replaced with unique tokens, preserving the original data's format and structure while concealing its actual value. Here's an example of how tokenization can be implementedin Python. \n",
    "\n",
    "\n",
    "In this example, the tokenize_data() function replaces each personal information entry (e.g., names, addresses, social security numbers) with a unique token generated using Python's uuid module. Each time the function is called, it generates a new UUID (Universally Unique Identifier), ensuring that each token is unique and not tied to any specific individual.\n",
    "\n",
    "By tokenizing the data, sensitive information is replaced with random tokens, preserving the dataset's structure while protecting the privacy of individuals. This technique is commonly used in data anonymization to enhance privacy and security when sharing or analyzing sensitive data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aa1def1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import uuid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "461e4894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Dataset:\n",
      "                                   Name                               Address  \\\n",
      "0  7a9ecaf9-87e4-4071-bd74-ef8eb555083a  53c984f9-4ea3-47c8-8c8f-c9201019a671   \n",
      "1  c9dcb003-c453-4c18-b4f6-38130e25d8ea  792d42e0-347b-4f50-a873-6534dfddf2d6   \n",
      "2  4a5da139-4401-4b76-a01a-0101fbcd77c1  db52b72f-d5f6-4777-a86e-60bb96b25eca   \n",
      "\n",
      "                 Social Security Number  \n",
      "0  be5686ba-4794-4fc1-9188-32864e701015  \n",
      "1  c0ab5763-ba32-492f-993a-1e52e3151230  \n",
      "2  941ca5cb-8001-4147-879c-9268a64c570a  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample dataset with personal information\n",
    "data = {\n",
    "    'Name': ['John Smith', 'Jane Doe', 'Alice Johnson'],\n",
    "    'Address': ['123 Main St', '456 Elm St', '789 Oak St'],\n",
    "    'Social Security Number': ['123-45-6789', '987-65-4321', '456-78-9123']\n",
    "}\n",
    "\n",
    "# Load dataset into a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define a function to tokenize personal information\n",
    "def tokenize_data(df):\n",
    "    # Generate unique tokens for each personal information entry\n",
    "    df['Name'] = [str(uuid.uuid4()) for _ in range(len(df))]\n",
    "    df['Address'] = [str(uuid.uuid4()) for _ in range(len(df))]\n",
    "    df['Social Security Number'] = [str(uuid.uuid4()) for _ in range(len(df))]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Tokenize personal information\n",
    "tokenized_df = tokenize_data(df)\n",
    "\n",
    "# Display tokenized dataset\n",
    "print(\"Tokenized Dataset:\")\n",
    "print(tokenized_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4d0600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6d45d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c16bcde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
