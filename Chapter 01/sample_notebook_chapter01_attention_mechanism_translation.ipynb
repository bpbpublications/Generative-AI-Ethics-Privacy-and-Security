{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ff69f77",
   "metadata": {},
   "source": [
    "# Translation using Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3709480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50861d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl --quiet\n",
    "!pip install https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.5.0/fr_core_news_sm-3.5.0-py3-none-any.whl --quiet\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9714df29",
   "metadata": {},
   "source": [
    "Here we import the necessary libraries and modules. \n",
    "- torch is the main PyTorch library\n",
    "- nn is the PyTorch module for building neural networks\n",
    "- F is a module containing PyTorch's functional operations\n",
    "- get_tokenizer is a function from torchtext that we'll use to tokenize (split) our input sentences into individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f56bf38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdd5bdb",
   "metadata": {},
   "source": [
    "Here, we define tokenizers for English and French using the SpaCy tokenizers. The get_tokenizer function takes two arguments: the tokenizer type ('spacy') and the language model to use ('en_core_web_sm' for English and 'fr_core_news_sm' for French).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ef8ae8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "# Define tokenizers for English and French\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "fr_tokenizer = get_tokenizer('spacy', language='fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bee5e0",
   "metadata": {},
   "source": [
    "We define vocabularies for English and French. These are dictionaries that map words to unique numerical indices. The <pad> token is a special token used for padding sequences to a fixed length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98e24665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vocabularies\n",
    "en_vocab = {'<pad>': 0, 'i': 1, 'want': 2, 'to': 3, 'learn': 4, 'about': 5, 'transformers': 6}\n",
    "fr_vocab = {'<pad>': 0, 'je': 1, 'veux': 2, 'apprendre': 3, 'sur': 4, 'les': 5, 'transformers': 6}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790f41a1",
   "metadata": {},
   "source": [
    "This function converts a sentence into a PyTorch tensor suitable for input to the model. It takes three arguments: the sentence, the vocabulary, and the tokenizer. \n",
    "\n",
    "Here's what it does:\n",
    "\n",
    "1. It tokenizes the sentence using the provided tokenizer, converting it to lowercase.\n",
    "2. For each token in the tokenized sentence, it looks up its index in the vocabulary. If the token is not found, it uses the index of the <pad> token.\n",
    "3. It converts the list of indices into a PyTorch tensor with data type torch.long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29593865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert sentence to tensor\n",
    "def sentence_to_tensor(sentence, vocab, tokenizer):\n",
    "    tokens = tokenizer(sentence.lower())\n",
    "    indices = [vocab.get(token, vocab['<pad>']) for token in tokens]\n",
    "    tensor = torch.tensor(indices, dtype=torch.long)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c460ea",
   "metadata": {},
   "source": [
    "We define the dimensionality of the word embeddings (4 in this example) and create embedding layers for English and French. \n",
    "\n",
    "The nn.Embedding module takes three arguments: \n",
    "1. The size of the vocabulary\n",
    "2. The dimensionality of the embeddings\n",
    "3. The index of the padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7167fa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding layers\n",
    "embedding_dim = 4\n",
    "en_embedding = nn.Embedding(len(en_vocab), embedding_dim, padding_idx=0)\n",
    "fr_embedding = nn.Embedding(len(fr_vocab), embedding_dim, padding_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8a5fa9",
   "metadata": {},
   "source": [
    "This is the ScaledDotProductAttention module, which implements the scaled dot-product attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd692c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / self.d_k ** 0.5\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attended_output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        return attended_output, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fda8096",
   "metadata": {},
   "source": [
    "The Encoder module takes an input sequence x and applies the attention mechanism to it. Here's what it does:\n",
    "\n",
    "1. It passes the input sequence x through the embedding layer to get word embeddings\n",
    "2. It applies the scaled dot-product attention mechanism to the embeddings, using the embeddings as queries, keys, and values\n",
    "3. It returns the attended output from the attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7777533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding, d_k):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = embedding\n",
    "        self.attention = ScaledDotProductAttention(d_k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x)\n",
    "        attended_output, _ = self.attention(embeddings, embeddings, embeddings)\n",
    "        return attended_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062eb999",
   "metadata": {},
   "source": [
    "The Decoder module takes an input sequence x and the output of the encoder. \n",
    "\n",
    "Here's what it does:\n",
    "\n",
    "1. It passes the input sequence x through the embedding layer to get word embeddings\n",
    "2. It applies the scaled dot-product attention mechanism to the embeddings, using the embeddings as queries, and the encoder output as keys and values\n",
    "3. It returns the attended output from the attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "010fddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding, d_k):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = embedding\n",
    "        self.attention = ScaledDotProductAttention(d_k)\n",
    "\n",
    "    def forward(self, x, encoder_output):\n",
    "        embeddings = self.embedding(x)\n",
    "        attended_output, _ = self.attention(embeddings, encoder_output, encoder_output)\n",
    "        return attended_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf7a1a0",
   "metadata": {},
   "source": [
    "The Transformer module combines the Encoder and Decoder modules. \n",
    "\n",
    "Here's what it does:\n",
    "\n",
    "1. In the constructor, it creates instances of the Encoder and Decoder modules, passing in the appropriate embedding layers and the dimensionality of the attention mechanism\n",
    "2. In the forward method, it takes an English sentence en_sentence and a French sentence fr_sentence as input\n",
    "3. It converts the input sentences to tensors using the sentence_to_tensor function and the appropriate vocabularies and tokenizers\n",
    "4. It passes the English sentence tensor through the Encoder module to get the encoder output\n",
    "5. It passes the French sentence tensor and the encoder output through the Decoder module to get the decoder output\n",
    "6. It returns the decoder output, which should ideally be similar to the embedding of the French sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a23d546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, en_vocab_size, fr_vocab_size, d_k):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(en_embedding, d_k)\n",
    "        self.decoder = Decoder(fr_embedding, d_k)\n",
    "\n",
    "    def forward(self, en_sentence, fr_sentence):\n",
    "        en_tensor = sentence_to_tensor(en_sentence, en_vocab, en_tokenizer)\n",
    "        fr_tensor = sentence_to_tensor(fr_sentence, fr_vocab, fr_tokenizer)\n",
    "\n",
    "        encoder_output = self.encoder(en_tensor)\n",
    "        decoder_output = self.decoder(fr_tensor, encoder_output)\n",
    "\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51399efe",
   "metadata": {},
   "source": [
    "In the example below, we create an instance of the Transformer model, passing in the sizes of the English and French vocabularies, and the dimensionality of the attention mechanism. We define an English sentence and its French translation, pass them through the model, and print the output (which should be similar to the embedding of the French sentence).\n",
    "\n",
    "\n",
    "##### Note that this is a very simplified example, and in practice, you would need to add additional components (like a linear layer to generate the output tokens), handle out-of-vocabulary words, and train the model on a large dataset of English-French sentence pairs to learn the translation task effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61913bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2523, -0.6888,  0.5728,  0.1789],\n",
      "        [ 0.1916, -0.8131,  0.5783,  0.2957],\n",
      "        [ 0.6950, -0.9435,  0.5721,  0.3822],\n",
      "        [ 0.6151, -0.9133,  0.5673,  0.3577],\n",
      "        [ 0.8427, -0.9631,  0.5620,  0.4115],\n",
      "        [ 0.4276, -0.8694,  0.5754,  0.3498]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "model = Transformer(len(en_vocab), len(fr_vocab), d_k=embedding_dim)\n",
    "\n",
    "en_sentence = \"I want to learn about transformers\"\n",
    "fr_sentence = \"je veux apprendre sur les transformers\"\n",
    "\n",
    "output = model(en_sentence, fr_sentence)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a508ff90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6575b987",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
