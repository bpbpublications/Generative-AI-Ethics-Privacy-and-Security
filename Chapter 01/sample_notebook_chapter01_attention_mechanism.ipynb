{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93740ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchtext--quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbd4e96",
   "metadata": {},
   "source": [
    "# Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d593925f",
   "metadata": {},
   "source": [
    "Here, we import PyTorch (a popular machine learning library). \n",
    "Two submodules from PyTorch (nn for building neural networks and nn.functional for common functions), and a function get_tokenizer from the torchtext library. We will use it to split the input sentence into individual words or tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12ab42eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6127415",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define tokenizer\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254d1efb",
   "metadata": {},
   "source": [
    "Function to convert a sentence into a tensor (a data structure that PyTorch can work with). The function does the following - \n",
    "\n",
    "1. It takes a sentence and a vocabulary (a dictionary that maps words to numbers) as input.\n",
    "2. It converts the sentence to lowercase and splits it into individual words (tokens) using the tokenizer function.\n",
    "3. For each token, it looks up its corresponding number in the vocabulary and adds it to a list called indices.\n",
    "4. It converts the list of numbers (indices) into a PyTorch tensor, which is the data structure we'll use for our computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "942420a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert sentence to tensor\n",
    "def sentence_to_tensor(sentence, vocab):\n",
    "    tokens = tokenizer(sentence.lower())\n",
    "    indices = [vocab[token] for token in tokens]\n",
    "    tensor = torch.tensor(indices, dtype=torch.long)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28614907",
   "metadata": {},
   "source": [
    "Then, we define the vocabulary. It is a dictionary that maps each word to a unique number. We'll use this to convert our sentences into tensors of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a81a5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vocabulary\n",
    "vocab = {'<pad>': 0, 'transformer': 1, 'architecture': 2, 'is': 3, 'amazing': 4, 'to': 5, 'learn': 6}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a43a06",
   "metadata": {},
   "source": [
    "Next, we create an embedding layer. This is a part of the neural network that converts the numbers representing words into vectors of numbers (embeddings). These embeddings are more useful for the network to work with than just the raw numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1084410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding layer\n",
    "embedding_dim = 4\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec719de",
   "metadata": {},
   "source": [
    "This is the core part of the attention mechanism. It takes in three inputs- \n",
    "\n",
    "1. q (queries)\n",
    "2. k (keys)\n",
    "3. v (values). \n",
    "\n",
    "The forward method computes the attention scores (how much the module should attend to different parts of the input), applies a mask if provided (to ignore certain parts of the input), and then computes the final attended output and attention weights.\n",
    "\n",
    "In the example usage section, \n",
    "- we take an input sentence: \"Transformer architecture is amazing to learn\"\n",
    "- Convert it to a tensor using the sentence_to_tensor function and the provided vocabulary.\n",
    "- Pass the tensor through the embedding layer to get embeddings (vector representations of the words).\n",
    "- Create an instance of the ScaledDotProductAttention module.\n",
    "- Compute the attended output and attention weights by passing the embeddings through the attention module.\n",
    "- Print the shapes of the attended output and attention weights.\n",
    "- Finally, we iterate over the tokens in the input sentence and print the attention weights for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32c65996",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / self.d_k ** 0.5\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attended_output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        return attended_output, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8710e627",
   "metadata": {},
   "source": [
    "Refer to the example now. When you run the code, you'll see that the attention mechanism assigns different weights (importance) to different words in the input sentence. For example, when processing the word \"transformer\", the model attends more to the words \"transformer\" itself and \"architecture\", and less to words like \"is\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "332ca8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attended Output Shape: torch.Size([1, 6, 4])\n",
      "Attention Weights Shape: torch.Size([1, 6, 6])\n",
      "transformer: tensor([0.4172, 0.1349, 0.1761, 0.0802, 0.0654, 0.1263],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "architecture: tensor([0.0868, 0.7531, 0.0112, 0.0640, 0.0247, 0.0603],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "is: tensor([0.0734, 0.0072, 0.7869, 0.0121, 0.0979, 0.0225],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "amazing: tensor([0.0335, 0.0416, 0.0121, 0.5409, 0.0941, 0.2778],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "to: tensor([0.0502, 0.0295, 0.1804, 0.1730, 0.4020, 0.1649],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "learn: tensor([0.0767, 0.0569, 0.0328, 0.4042, 0.1304, 0.2990],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "input_sentence = \"Transformer architecture is amazing to learn\"\n",
    "input_tensor = sentence_to_tensor(input_sentence, vocab)\n",
    "embeddings = embedding(input_tensor).unsqueeze(0)  # [1, seq_len, embedding_dim]\n",
    "\n",
    "# Create attention module\n",
    "attention = ScaledDotProductAttention(d_k=embedding_dim)\n",
    "\n",
    "# Compute attended output and attention weights\n",
    "attended_output, attn_weights = attention(embeddings, embeddings, embeddings)\n",
    "print(\"Attended Output Shape:\", attended_output.shape)\n",
    "print(\"Attention Weights Shape:\", attn_weights.shape)\n",
    "\n",
    "# Print attention weights for each word\n",
    "for i, token in enumerate(tokenizer(input_sentence.lower())):\n",
    "    print(f\"{token}: {attn_weights[0, i, :]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac3e59d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
