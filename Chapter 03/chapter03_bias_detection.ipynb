{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd8a230e",
   "metadata": {},
   "source": [
    "# Bias Detection\n",
    "\n",
    "Bias in a machine learning model is the error introduced by overly simplistic assumptions in the learning algorithm, leading to underfitting where the model fails to capture the underlying patterns in the data. It is a component of the bias-variance trade-off, where high bias results in a model that performs poorly on both training and unseen data. Balancing bias and variance is crucial for creating a model that generalizes well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a8dded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aif360 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ec4551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "972723b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d101f383",
   "metadata": {},
   "source": [
    "The `TextGenerationModel` class is a PyTorch module designed for text generation using an LSTM network. \n",
    "It initializes with four parameters: \n",
    "- `input_size` \n",
    "- `hidden_size`\n",
    "- `output_size`\n",
    "- `vocab_size`\n",
    "\n",
    "The class includes an embedding layer (`nn.Embedding`) to convert word indices into dense vectors, an LSTM layer (`nn.LSTM`) to process these sequences, and a fully connected layer (`nn.Linear`) to map the LSTM's hidden state output to the desired output size, which represents the vocabulary size for predicting the next word. \n",
    "\n",
    "In the `forward` method, the input tensor `x` is first passed through the embedding layer, then processed by the LSTM layer, and finally, the output of the LSTM's last time step is passed through the fully connected layer to generate the prediction for the next word in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d106d22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch module for text generation using LSTM.\n",
    "\n",
    "    Attributes:\n",
    "        input_size (int): The size of the input vocabulary.\n",
    "        hidden_size (int): The size of the hidden LSTM layer.\n",
    "        output_size (int): The size of the output vocabulary.\n",
    "        vocab_size (int): The size of the vocabulary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, vocab_size):\n",
    "        \"\"\"\n",
    "        Initializes the TextGenerationModel.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): The size of the input vocabulary.\n",
    "            hidden_size (int): The size of the hidden LSTM layer.\n",
    "            output_size (int): The size of the output vocabulary.\n",
    "            vocab_size (int): The size of the vocabulary.\n",
    "        \"\"\"\n",
    "        super(TextGenerationModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding(vocab_size, input_size)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor representing the text data.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor representing the generated text.\n",
    "        \"\"\"\n",
    "        x = self.embed(x)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024024bf",
   "metadata": {},
   "source": [
    "The `measure_fairness` function evaluates fairness metrics for a text generation model, particularly concerning a specified sensitive attribute such as gender or race. It accepts a model, a dataset comprising prompts, corresponding texts, and sensitive attribute values, along with a vocabulary mapping tokens to their embedding indices. \n",
    "\n",
    "For each data point, the function converts the prompt into a tensor using the vocabulary, feeds it to the model to generate outputs, and records whether the generated text matches the expected text. It also collects the sensitive attribute values. \n",
    "These outputs, labels, and sensitive attributes are then converted into numpy arrays and used to create a pandas DataFrame. This DataFrame is transformed into a `BinaryLabelDataset` for evaluating fairness metrics. The function specifically calculates the disparate impact metric, which measures the ratio of favorable outcomes between privileged and unprivileged groups. The result, encapsulated in a dictionary, is returned as the fairness metric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "927b52cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_fairness(model, data, sensitive_attr, vocab):\n",
    "    \"\"\"\n",
    "    Measure fairness metrics for a text generation model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The text generation model.\n",
    "        data (list): A list of tuples (prompt, text, sensitive_attr_value).\n",
    "        sensitive_attr (str): The sensitive attribute to measure fairness for (e.g., 'gender', 'race').\n",
    "        vocab (dict): A dictionary mapping tokens to their indices in the embedding layer.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing fairness metric values.\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    labels = []\n",
    "    sensitive_attrs = []\n",
    "\n",
    "    for prompt, text, attr_value in data:\n",
    "        prompt_tensor = torch.tensor([vocab[token] for token in prompt.split()], dtype=torch.long)\n",
    "        output = model(prompt_tensor.unsqueeze(0))\n",
    "        outputs.append(output.squeeze().detach().numpy())\n",
    "        labels.append(int(text in output_vocab))\n",
    "        sensitive_attrs.append(attr_value)\n",
    "\n",
    "    outputs = np.array(outputs)\n",
    "    labels = np.array(labels)\n",
    "    sensitive_attrs = np.array(sensitive_attrs)\n",
    "\n",
    "    df = pd.DataFrame({\"label\": labels, sensitive_attr: sensitive_attrs})\n",
    "\n",
    "    dataset = BinaryLabelDataset(\n",
    "        favorable_label=1,\n",
    "        unfavorable_label=0,\n",
    "        df=df,\n",
    "        label_names=['label'],\n",
    "        protected_attribute_names=[sensitive_attr],\n",
    "        unprivileged_protected_attributes=[{sensitive_attr: 1}],\n",
    "    )\n",
    "\n",
    "    dataset.scores = outputs\n",
    "\n",
    "    metric = BinaryLabelDatasetMetric(dataset, unprivileged_groups=[{sensitive_attr: 1}], privileged_groups=[{sensitive_attr: 0}])\n",
    "    disparate_impact = metric.disparate_impact()\n",
    "    fairness_metrics = {\n",
    "        \"Disparate Impact\": disparate_impact\n",
    "    }\n",
    "\n",
    "    return fairness_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8be3a5",
   "metadata": {},
   "source": [
    "In this code, a small dataset (`data`) is defined, consisting of four tuples where each tuple contains a prompt, a continuation text, and a binary sensitive attribute indicating gender (0 for male, 1 for female). \n",
    "\n",
    "- A vocabulary dictionary (`vocab`) maps words to their corresponding indices, and an output vocabulary (`output_vocab`) lists the possible generated text completions. \n",
    "\n",
    "- A text generation model (`TextGenerationModel`) is instantiated with specified sizes for input, hidden, and output layers, and the embedding layer's weights are initialized uniformly between -1 and 1. \n",
    "\n",
    "- The `measure_fairness` function is then called to evaluate the fairness of the model concerning gender. \n",
    "\n",
    "This function processes the data to generate model outputs, labels, and sensitive attribute values, calculates the disparate impact metric, and returns it in a dictionary. Finally, the calculated fairness metrics are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e717accb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Disparate Impact': 1.0}\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"The young wizard \", \"waved his wand and cast a powerful spell.\", 0),  # Male\n",
    "    (\"The enchantress \", \"conjured a magical potion from thin air.\", 1),  # Female\n",
    "    (\"The wise sorcerer \", \"consulted ancient tomes for forbidden knowledge.\", 0),  # Male\n",
    "    (\"The sorceress \", \"channeled the elements to bend them to her will.\", 1),  # Female\n",
    "]\n",
    "\n",
    "vocab = {'The': 0, 'young': 1, 'wizard': 2, 'waved': 3, 'his': 4, 'wand': 5, 'and': 6, 'cast': 7, 'a': 8, 'powerful': 9, 'spell.': 10, 'enchantress': 11, 'conjured': 12, 'magical': 13, 'potion': 14, 'from': 15, 'thin': 16, 'air.': 17, 'wise': 18, 'sorcerer': 19, 'consulted': 20, 'ancient': 21, 'tomes': 22, 'for': 23, 'forbidden': 24, 'knowledge.': 25, 'sorceress': 26, 'channeled': 27, 'the': 28, 'elements': 29, 'to': 30, 'bend': 31, 'them': 32, 'her': 33, 'will.': 34}\n",
    "output_vocab = ['waved his wand and cast a powerful spell.', 'conjured a magical potion from thin air.', 'consulted ancient tomes for forbidden knowledge.', 'channeled the elements to bend them to her will.']\n",
    "\n",
    "# Initialize model\n",
    "model = TextGenerationModel(input_size=10, hidden_size=20, output_size=len(output_vocab), vocab_size=len(vocab))\n",
    "model.embed.weight.data.uniform_(-1, 1)\n",
    "\n",
    "# Measure fairness\n",
    "fairness_metrics = measure_fairness(model, data, \"gender\", vocab)\n",
    "print(fairness_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539d1a8f",
   "metadata": {},
   "source": [
    "In the context of fairness metrics, the Disparate Impact (DI) ratio is a measure used to evaluate whether different groups (typically defined by sensitive attributes such as gender, race, etc.) receive favorable outcomes at different rates. It's calculated as the ratio of the rate of favorable outcomes for the unprivileged group to the rate of favorable outcomes for the privileged group.\n",
    "\n",
    "A Disparate Impact value of 1 indicates perfect equality between the groups being compared. Specifically, it means that the rate of favorable outcomes (e.g., correct text generation or correct predictions) is the same for both the unprivileged group and the privileged group. In other words, the model does not show any bias against either group according to this metric.\n",
    "\n",
    "\n",
    "- **Disparate Impact = 1**: The model treats both groups equally in terms of favorable outcomes.\n",
    "- **Disparate Impact < 1**: The unprivileged group receives favorable outcomes at a lower rate than the privileged group, indicating potential bias against the unprivileged group.\n",
    "- **Disparate Impact > 1**: The unprivileged group receives favorable outcomes at a higher rate than the privileged group, which could indicate bias in favor of the unprivileged group.\n",
    "\n",
    "In your case, a Disparate Impact of 1 suggests that your text generation model is fair with respect to the sensitive attribute being measured (gender in this case), as it provides equal favorable outcome rates for both male and female attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d19ae2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Disparate Impact': 0.5}\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"The young wizard \", \"waved his wand and cast a powerful spell.\", 0),  # Male\n",
    "    (\"The enchantress \", \"conjured a magical potion from thin air.\", 1),  # Female\n",
    "    (\"The wise sorcerer \", \"consulted ancient tomes for forbidden knowledge.\", 0),  # Male\n",
    "    (\"The sorceress \", \"attempted to cast a spell but failed.\", 1),  # Female - modified text\n",
    "]\n",
    "\n",
    "# Initialize model\n",
    "model = TextGenerationModel(input_size=10, hidden_size=20, output_size=len(output_vocab), vocab_size=len(vocab))\n",
    "model.embed.weight.data.uniform_(-1, 1)\n",
    "\n",
    "# Measure fairness\n",
    "fairness_metrics = measure_fairness(model, data, \"gender\", vocab)\n",
    "print(fairness_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d0ce49",
   "metadata": {},
   "source": [
    "A Disparate Impact (DI) value of 0.5 in this context means that the rate of favorable outcomes for the unprivileged group (in this case, likely the female gender group with the sensitive attribute value of 1) is half that of the privileged group (the male gender group with the sensitive attribute value of 0). This suggests potential bias in the model's predictions, favoring the privileged group (males) over the unprivileged group (females).\n",
    "\n",
    "Let's break this down with the given data and your modified example:\n",
    "\n",
    "### Data Breakdown\n",
    "- **Male group (sensitive attribute value 0)**:\n",
    "  1. Prompt: \"The young wizard \", Expected output: \"waved his wand and cast a powerful spell.\"\n",
    "  2. Prompt: \"The wise sorcerer \", Expected output: \"consulted ancient tomes for forbidden knowledge.\"\n",
    "\n",
    "- **Female group (sensitive attribute value 1)**:\n",
    "  1. Prompt: \"The enchantress \", Expected output: \"conjured a magical potion from thin air.\"\n",
    "  2. Prompt: \"The sorceress \", Expected output: \"attempted to cast a spell but failed.\" (modified text)\n",
    "\n",
    "### Interpretation of Disparate Impact 0.5\n",
    "In the context of the Disparate Impact metric:\n",
    "- If the model correctly predicts the expected output more frequently for one group over the other, it indicates bias.\n",
    "- A DI of 0.5 means the female group's favorable outcomes rate is 50% of the male group's rate.\n",
    "\n",
    "### Example Scenario\n",
    "- Suppose the model correctly generates the expected output for the male group twice (both instances).\n",
    "- Suppose it correctly generates the expected output for the female group once.\n",
    "\n",
    "The favorable outcome rate for each group might look something like this:\n",
    "- Male group: 2 correct out of 2 instances (100%)\n",
    "- Female group: 1 correct out of 2 instances (50%)\n",
    "\n",
    "Disparate Impact calculation:\n",
    "\\[ \\text{DI} = \\frac{\\text{Favorable rate for females}}{\\text{Favorable rate for males}} = \\frac{0.5}{1.0} = 0.5 \\]\n",
    "\n",
    "This indicates that females receive favorable outcomes at half the rate of males, highlighting a bias in the model's predictions against the female group.\n",
    "\n",
    "### Conclusion\n",
    "A DI value of 0.5 reveals that the model's performance is biased, providing favorable outcomes to females only half as often as it does to males. This suggests that the model might require further training, adjustment, or re-evaluation to ensure fair treatment across both gender groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baa0fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
