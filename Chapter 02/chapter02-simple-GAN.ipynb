{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4281a7b",
   "metadata": {},
   "source": [
    "# Generate synthetic data using GAN\n",
    "\n",
    "Here's a simple example of how you can implement a generative adversarial network (GAN) using Python and PyTorch to generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b05ea36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision dataloader --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47456d12",
   "metadata": {},
   "source": [
    "We import the necessary libraries for loading the MNIST dataset. We import the Python modules and libraries necessary. \n",
    "\n",
    "- torch is the main PyTorch library for tensor computations and neural network operations. \n",
    "- torch.nn provides classes and functions for building neural networks. \n",
    "- torchvision.datasets contains popular datasets like MNIST \n",
    "- torchvision.transforms provides utilities for data transformations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5511f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84d0615",
   "metadata": {},
   "source": [
    "This block defines the Generator network as a PyTorch module. \n",
    "\n",
    "The __init__ method initializes the network with the specified latent_dim (size of the random noise vector) and output_dim (size of the output data, e.g., 784 for MNIST images). \n",
    "\n",
    "The Generator network is a simple feed-forward neural network with two linear layers, a LeakyReLU activation, and a Tanh activation at the output. The forward method takes a random noise vector z as input and passes it through the network to generate synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fef4f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Generator Network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.generator = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a5f4a",
   "metadata": {},
   "source": [
    "This block defines the Discriminator network as a PyTorch module. \n",
    "\n",
    "The __init__ method initializes the network with the input_dim (size of the input data). \n",
    "\n",
    "The Discriminator network is also a simple feed-forward neural network with two linear layers, a LeakyReLU activation, and a Sigmoid activation at the output. The forward method takes input data x (either real or synthetic) and passes it through the network to produce a scalar output between 0 and 1, representing the probability that the input data is real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad01f1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Discriminator Network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.discriminator(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17fcfe3",
   "metadata": {},
   "source": [
    "Set the hyperparameters for the GAN: latent_dim is the size of the random noise vector, and output_dim is the size of the output data (784 for MNIST images, which are 28x28 pixels). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25c574c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "latent_dim = 64\n",
    "output_dim = 784  # For MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7ccb53",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9aec44b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the networks\n",
    "generator = Generator(latent_dim, output_dim)\n",
    "discriminator = Discriminator(output_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a72aacf",
   "metadata": {},
   "source": [
    "The loss function and optimizers for training the GAN:\n",
    "\n",
    "- nn.BCELoss is the Binary Cross-Entropy loss, which is typically used for binary classification tasks like discriminating between real and fake data. \n",
    "\n",
    "- The Adam optimizer is used for both the Generator and Discriminator networks, with a learning rate of 0.0002."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "003aa24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "gen_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002)\n",
    "disc_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fea3ae5",
   "metadata": {},
   "source": [
    "First, a transformation is defined to convert the images to PyTorch tensors and normalize them to have a mean of 0.5 and a standard deviation of 0.5. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "540467aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f6e70e",
   "metadata": {},
   "source": [
    "The datasets.MNIST function downloads and loads the MNIST dataset from the specified root directory (here, ./data). \n",
    "The train=True argument indicates that we want to load the training set. The download=True argument ensures that the dataset is downloaded if it is not already present in the root directory. The dataloader is created from the train_dataset with a batch size of 128 and shuffling enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89cf341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcbacfd",
   "metadata": {},
   "source": [
    "The training loop runs for num_epochs iterations (50 in this case). In each epoch, the dataloader is iterated over to obtain batches of real MNIST images (real_data).\n",
    "\n",
    "Train the Discriminator:\n",
    "\n",
    "- Reset the Discriminator's optimizer gradients.\n",
    "- Create labels for real and fake data.\n",
    "- Pass real data through the Discriminator to get its output.\n",
    "- Calculate loss comparing Discriminator's output with real labels.\n",
    "- Generate random noise for fake data.\n",
    "- Generate fake data using the Generator.\n",
    "- Pass fake data through the Discriminator to get its output.\n",
    "- Calculate loss comparing Discriminator's output with fake labels.\n",
    "- Calculate total loss as the average of real and fake losses.\n",
    "- Backpropagate the loss and update Discriminator's weights.\n",
    "\n",
    "Train the Generator:\n",
    "\n",
    "- Reset the Generator's optimizer gradients.\n",
    "- Generate new random noise.\n",
    "- Generate fake data using the Generator.\n",
    "- Pass fake data through the Discriminator to get its output.\n",
    "- Calculate loss comparing Discriminator's output with real labels.\n",
    "- Backpropagate the loss and update Generator's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41b5fa77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Generator Loss: 1.4403, Discriminator Loss: 0.4601\n",
      "Epoch [2/50], Generator Loss: 1.6008, Discriminator Loss: 0.4184\n",
      "Epoch [3/50], Generator Loss: 1.8119, Discriminator Loss: 0.2253\n",
      "Epoch [4/50], Generator Loss: 0.9599, Discriminator Loss: 0.5667\n",
      "Epoch [5/50], Generator Loss: 0.9471, Discriminator Loss: 0.6062\n",
      "Epoch [6/50], Generator Loss: 0.8735, Discriminator Loss: 0.6687\n",
      "Epoch [7/50], Generator Loss: 1.2985, Discriminator Loss: 0.5231\n",
      "Epoch [8/50], Generator Loss: 1.0226, Discriminator Loss: 0.5434\n",
      "Epoch [9/50], Generator Loss: 1.2984, Discriminator Loss: 0.4454\n",
      "Epoch [10/50], Generator Loss: 1.6485, Discriminator Loss: 0.3565\n",
      "Epoch [11/50], Generator Loss: 1.4496, Discriminator Loss: 0.4481\n",
      "Epoch [12/50], Generator Loss: 1.2997, Discriminator Loss: 0.4426\n",
      "Epoch [13/50], Generator Loss: 1.4082, Discriminator Loss: 0.4128\n",
      "Epoch [14/50], Generator Loss: 1.1078, Discriminator Loss: 0.5760\n",
      "Epoch [15/50], Generator Loss: 1.3003, Discriminator Loss: 0.4328\n",
      "Epoch [16/50], Generator Loss: 1.3367, Discriminator Loss: 0.4262\n",
      "Epoch [17/50], Generator Loss: 1.1358, Discriminator Loss: 0.5337\n",
      "Epoch [18/50], Generator Loss: 0.9256, Discriminator Loss: 0.6843\n",
      "Epoch [19/50], Generator Loss: 0.9813, Discriminator Loss: 0.7255\n",
      "Epoch [20/50], Generator Loss: 1.1204, Discriminator Loss: 0.5418\n",
      "Epoch [21/50], Generator Loss: 1.0059, Discriminator Loss: 0.5558\n",
      "Epoch [22/50], Generator Loss: 1.3744, Discriminator Loss: 0.4094\n",
      "Epoch [23/50], Generator Loss: 0.8287, Discriminator Loss: 0.6942\n",
      "Epoch [24/50], Generator Loss: 1.0086, Discriminator Loss: 0.5409\n",
      "Epoch [25/50], Generator Loss: 1.3040, Discriminator Loss: 0.5626\n",
      "Epoch [26/50], Generator Loss: 1.4287, Discriminator Loss: 0.4863\n",
      "Epoch [27/50], Generator Loss: 1.3345, Discriminator Loss: 0.5081\n",
      "Epoch [28/50], Generator Loss: 1.2510, Discriminator Loss: 0.4016\n",
      "Epoch [29/50], Generator Loss: 1.6117, Discriminator Loss: 0.3631\n",
      "Epoch [30/50], Generator Loss: 1.9358, Discriminator Loss: 0.4152\n",
      "Epoch [31/50], Generator Loss: 1.4603, Discriminator Loss: 0.4379\n",
      "Epoch [32/50], Generator Loss: 1.2480, Discriminator Loss: 0.4697\n",
      "Epoch [33/50], Generator Loss: 1.3311, Discriminator Loss: 0.5170\n",
      "Epoch [34/50], Generator Loss: 1.5704, Discriminator Loss: 0.4698\n",
      "Epoch [35/50], Generator Loss: 1.6717, Discriminator Loss: 0.5479\n",
      "Epoch [36/50], Generator Loss: 1.4680, Discriminator Loss: 0.4326\n",
      "Epoch [37/50], Generator Loss: 1.2944, Discriminator Loss: 0.5481\n",
      "Epoch [38/50], Generator Loss: 1.1767, Discriminator Loss: 0.5731\n",
      "Epoch [39/50], Generator Loss: 1.4637, Discriminator Loss: 0.5782\n",
      "Epoch [40/50], Generator Loss: 1.5045, Discriminator Loss: 0.4305\n",
      "Epoch [41/50], Generator Loss: 1.2919, Discriminator Loss: 0.5362\n",
      "Epoch [42/50], Generator Loss: 1.0222, Discriminator Loss: 0.5587\n",
      "Epoch [43/50], Generator Loss: 1.2543, Discriminator Loss: 0.6573\n",
      "Epoch [44/50], Generator Loss: 1.3568, Discriminator Loss: 0.5818\n",
      "Epoch [45/50], Generator Loss: 1.4097, Discriminator Loss: 0.5120\n",
      "Epoch [46/50], Generator Loss: 1.3504, Discriminator Loss: 0.5147\n",
      "Epoch [47/50], Generator Loss: 1.3204, Discriminator Loss: 0.5423\n",
      "Epoch [48/50], Generator Loss: 1.2856, Discriminator Loss: 0.5173\n",
      "Epoch [49/50], Generator Loss: 1.3494, Discriminator Loss: 0.5066\n",
      "Epoch [50/50], Generator Loss: 1.2764, Discriminator Loss: 0.5663\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (real_data, _) in enumerate(dataloader):\n",
    "        # Train the discriminator\n",
    "        disc_optimizer.zero_grad()\n",
    "        real_label = torch.ones(real_data.size(0), 1)\n",
    "        fake_label = torch.zeros(real_data.size(0), 1)\n",
    "        real_output = discriminator(real_data.view(-1, output_dim))\n",
    "        real_loss = criterion(real_output, real_label)\n",
    "        z = torch.randn(real_data.size(0), latent_dim)\n",
    "        fake_data = generator(z)\n",
    "        fake_output = discriminator(fake_data)\n",
    "        fake_loss = criterion(fake_output, fake_label)\n",
    "        disc_loss = (real_loss + fake_loss) / 2\n",
    "        disc_loss.backward()\n",
    "        disc_optimizer.step()\n",
    "        # Train the generator\n",
    "        gen_optimizer.zero_grad()\n",
    "        z = torch.randn(real_data.size(0), latent_dim)\n",
    "        fake_data = generator(z)\n",
    "        fake_output = discriminator(fake_data)\n",
    "        gen_loss = criterion(fake_output, real_label)\n",
    "        gen_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "\n",
    "    # Print losses and generate synthetic data\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Generator Loss: {gen_loss.item():.4f}, Discriminator Loss: {disc_loss.item():.4f}')\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(16, latent_dim)\n",
    "        synthetic_data = generator(z).view(-1, 1, 28, 28)\n",
    "        # Save or visualize the generated synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbff6665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28356a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2776c72d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
